#!/usr/bin/env python3
"""
æç¤ºè¯ç®¡ç†å™¨æ¼”ç¤ºè„šæœ¬ - æ¨¡æ‹Ÿå®é™…ä½¿ç”¨åœºæ™¯
"""

import json
from prompt_manager import smart_get_prompt, get_available_prompts

def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "="*60)
    print(f"ğŸ¯ {title}")
    print("="*60)

def demo_interactive_mode():
    """äº¤äº’å¼æ¼”ç¤ºæ¨¡å¼"""
    print_header("äº¤äº’å¼æç¤ºè¯æ™ºèƒ½é€‰æ‹©æ¼”ç¤º")
    
    print("ğŸ’¡ è¾“å…¥å€ºåˆ¸äº¤æ˜“æ–‡æœ¬ï¼ŒAIå°†è‡ªåŠ¨é€‰æ‹©æœ€é€‚åˆçš„æç¤ºè¯æ¨¡æ¿")
    print("ğŸ’¡ è¾“å…¥ 'quit' é€€å‡ºï¼Œè¾“å…¥ 'help' æŸ¥çœ‹å¯ç”¨æ¨¡æ¿")
    
    while True:
        print("\n" + "-"*40)
        user_input = input("ğŸ“ è¯·è¾“å…¥å€ºåˆ¸äº¤æ˜“æ–‡æœ¬: ").strip()
        
        if user_input.lower() == 'quit':
            print("ğŸ‘‹ å†è§ï¼")
            break
        elif user_input.lower() == 'help':
            available = get_available_prompts()
            print("\nğŸ“‹ å¯ç”¨æç¤ºè¯æ¨¡æ¿:")
            for prompt_type in available['available_prompts']:
                description = available['descriptions'][prompt_type]
                print(f"  - {prompt_type}: {description}")
            continue
        elif not user_input:
            print("âŒ è¯·è¾“å…¥æœ‰æ•ˆçš„æ–‡æœ¬")
            continue
        
        try:
            # ä½¿ç”¨æ™ºèƒ½æç¤ºè¯ç®¡ç†å™¨
            result = smart_get_prompt(user_input)
            
            print(f"\nğŸ¤– AIåˆ†æç»“æœ:")
            print(f"ğŸ“Š æ¨èæ¨¡æ¿: {result['recommended_prompt_type']}")
            print(f"ğŸ’¡ æ¨ç†è¿‡ç¨‹: {result['reasoning']}")
            
            print(f"\nğŸ“ ç”Ÿæˆçš„æç¤ºè¯:")
            print("-" * 40)
            print(result['filled_prompt'])
            print("-" * 40)
            
            print(f"\nğŸ“ æç¤ºè¯é•¿åº¦: {len(result['filled_prompt'])} å­—ç¬¦")
            
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {str(e)}")

def demo_batch_processing():
    """æ‰¹é‡å¤„ç†æ¼”ç¤º"""
    print_header("æ‰¹é‡å¤„ç†æ¼”ç¤º")
    
    # æ¨¡æ‹Ÿå®é™…ä¸šåŠ¡åœºæ™¯çš„æµ‹è¯•æ•°æ®
    test_scenarios = [
        {
            "scenario": "æ™®é€šå•ä¸€å€ºåˆ¸äº¤æ˜“",
            "texts": [
                "ä¹°å…¥24å›½å€º07 1000ä¸‡ 3.5%",
                "å–å‡º25å›½å€º08 500ä¸‡ 3.2%"
            ]
        },
        {
            "scenario": "åŒ…å«è¯·ç¤ºæ ‡è®°çš„äº¤æ˜“",
            "texts": [
                "ä¹°å…¥24å›½å€º07 *1000ä¸‡ 3.5%",
                "å–å‡º25å›½å€º08 500ä¸‡ *3.2%"
            ]
        },
        {
            "scenario": "å¤šäº§å“å¤åˆäº¤æ˜“",
            "texts": [
                "ä¹°å…¥24å›½å€º07 1000ä¸‡ 3.5%ï¼Œå–å‡º25å›½å€º08 2000ä¸‡ 3.8%",
                "æ™¯ç›Šè´§å¸åŸºé‡‘ä¹°å…¥24å›½å€º07 1000ä¸‡ 3.5%ï¼Œæ™¯é¡ºè´§å¸åŸºé‡‘å–å‡º25å›½å€º08 500ä¸‡ 3.2%"
            ]
        },
        {
            "scenario": "å¯è®®ä»·äº¤æ˜“",
            "texts": [
                "ä¹°å…¥24å›½å€º07 1000ä¸‡ ä»·æ ¼å¯è®®",
                "å–å‡º25å›½å€º08 500ä¸‡ å¯è®®ä»·"
            ]
        },
        {
            "scenario": "é—®ç­”å’¨è¯¢",
            "texts": [
                "ä»€ä¹ˆæ˜¯å€ºåˆ¸äº¤æ˜“ï¼Ÿ",
                "å€ºåˆ¸äº¤æ˜“æœ‰å“ªäº›é£é™©ï¼Ÿ",
                "å¦‚ä½•è®¡ç®—å€ºåˆ¸æ”¶ç›Šç‡ï¼Ÿ"
            ]
        }
    ]
    
    for scenario in test_scenarios:
        print(f"\nğŸ¬ åœºæ™¯: {scenario['scenario']}")
        print("-" * 50)
        
        for i, text in enumerate(scenario['texts'], 1):
            print(f"\nğŸ“ ç¤ºä¾‹ {i}: {text}")
            
            try:
                result = smart_get_prompt(text)
                print(f"   ğŸ¯ æ¨èæ¨¡æ¿: {result['recommended_prompt_type']}")
                print(f"   ğŸ“Š ç‰¹å¾åˆ†æ: åŒ…å«*å·={result['text_characteristics']['has_asterisk']}, "
                      f"å¤šäº§å“={result['text_characteristics']['has_multiple_products']}, "
                      f"æ˜¯é—®é¢˜={result['text_characteristics']['is_chat_question']}")
                
            except Exception as e:
                print(f"   âŒ å¤„ç†å¤±è´¥: {str(e)}")

def demo_api_simulation():
    """æ¨¡æ‹ŸAPIè°ƒç”¨æ¼”ç¤º"""
    print_header("APIè°ƒç”¨æ¨¡æ‹Ÿæ¼”ç¤º")
    
    print("ğŸ”§ æ¨¡æ‹Ÿå°†æç¤ºè¯ç®¡ç†å™¨é›†æˆåˆ°ç°æœ‰APIä¸­...")
    
    def simulate_api_call(text, context=""):
        """æ¨¡æ‹ŸAPIè°ƒç”¨"""
        print(f"\nğŸ“¡ APIè°ƒç”¨: /smart-parse")
        print(f"ğŸ“¥ è¾“å…¥å‚æ•°: text='{text}', context='{context}'")
        
        try:
            # ä½¿ç”¨æ™ºèƒ½æç¤ºè¯ç®¡ç†å™¨
            result = smart_get_prompt(text, context)
            
            # æ¨¡æ‹ŸAPIå“åº”
            api_response = {
                "status": "success",
                "data": {
                    "recommended_prompt_type": result['recommended_prompt_type'],
                    "prompt": result['filled_prompt'],
                    "analysis": result['text_characteristics'],
                    "reasoning": result['reasoning']
                }
            }
            
            print(f"ğŸ“¤ APIå“åº”: {json.dumps(api_response, ensure_ascii=False, indent=2)}")
            return api_response
            
        except Exception as e:
            error_response = {
                "status": "error",
                "message": str(e)
            }
            print(f"ğŸ“¤ APIå“åº”: {json.dumps(error_response, ensure_ascii=False, indent=2)}")
            return error_response
    
    # æ¨¡æ‹Ÿå‡ ä¸ªAPIè°ƒç”¨
    test_calls = [
        {"text": "ä¹°å…¥24å›½å€º07 *1000ä¸‡ 3.5%"},
        {"text": "ä»€ä¹ˆæ˜¯å€ºåˆ¸äº¤æ˜“ï¼Ÿ", "context": "ç”¨æˆ·æ˜¯æ–°æ‰‹æŠ•èµ„è€…"},
        {"text": "ä¹°å…¥24å›½å€º07 1000ä¸‡ 3.5%ï¼Œå–å‡º25å›½å€º08 2000ä¸‡ 3.8%"}
    ]
    
    for call in test_calls:
        simulate_api_call(**call)

def demo_integration_example():
    """é›†æˆç¤ºä¾‹æ¼”ç¤º"""
    print_header("é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿç¤ºä¾‹")
    
    print("ğŸ”§ æ¼”ç¤ºå¦‚ä½•å°†æ™ºèƒ½æç¤ºè¯ç®¡ç†å™¨é›†æˆåˆ°ç°æœ‰çš„å€ºåˆ¸è§£æç³»ç»Ÿä¸­...")
    
    # æ¨¡æ‹ŸåŸæœ‰ç³»ç»Ÿçš„å‡½æ•°
    def old_parse_bond_text(text, prompt_type="å•ä¸€æŒ‡ä»¤è§£æ"):
        """åŸæœ‰çš„è§£æå‡½æ•°ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        print(f"ğŸ”„ åŸæœ‰ç³»ç»Ÿ: ä½¿ç”¨å›ºå®šæç¤ºè¯ç±»å‹ '{prompt_type}' è§£ææ–‡æœ¬")
        return f"ä½¿ç”¨ {prompt_type} è§£æ: {text}"
    
    def new_parse_bond_text(text, context=""):
        """æ–°çš„æ™ºèƒ½è§£æå‡½æ•°"""
        print(f"ğŸš€ æ–°ç³»ç»Ÿ: ä½¿ç”¨AIæ™ºèƒ½é€‰æ‹©æç¤ºè¯ç±»å‹")
        
        # ä½¿ç”¨æ™ºèƒ½æç¤ºè¯ç®¡ç†å™¨
        result = smart_get_prompt(text, context)
        
        print(f"   ğŸ¯ AIæ¨è: {result['recommended_prompt_type']}")
        print(f"   ğŸ’¡ æ¨ç†: {result['reasoning']}")
        
        # è¿™é‡Œå¯ä»¥ç»§ç»­è°ƒç”¨å®é™…çš„æ¨¡å‹è§£æ
        return {
            "prompt_type": result['recommended_prompt_type'],
            "prompt": result['filled_prompt'],
            "analysis": result['text_characteristics']
        }
    
    # å¯¹æ¯”æ¼”ç¤º
    test_text = "ä¹°å…¥24å›½å€º07 *1000ä¸‡ 3.5%"
    
    print(f"\nğŸ“ æµ‹è¯•æ–‡æœ¬: {test_text}")
    print("\nğŸ”„ åŸæœ‰æ–¹å¼:")
    old_result = old_parse_bond_text(test_text)
    print(f"   ç»“æœ: {old_result}")
    
    print("\nğŸš€ æ–°çš„æ™ºèƒ½æ–¹å¼:")
    new_result = new_parse_bond_text(test_text)
    print(f"   ç»“æœ: æ™ºèƒ½é€‰æ‹©äº† '{new_result['prompt_type']}' æ¨¡æ¿")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ‰ å€ºåˆ¸äº¤æ˜“æç¤ºè¯æ™ºèƒ½ç®¡ç†å™¨æ¼”ç¤º")
    print("=" * 60)
    
    while True:
        print("\nğŸ“‹ è¯·é€‰æ‹©æ¼”ç¤ºæ¨¡å¼:")
        print("1. äº¤äº’å¼æ¼”ç¤º (æ¨è)")
        print("2. æ‰¹é‡å¤„ç†æ¼”ç¤º")
        print("3. APIè°ƒç”¨æ¨¡æ‹Ÿ")
        print("4. ç³»ç»Ÿé›†æˆç¤ºä¾‹")
        print("5. é€€å‡º")
        
        choice = input("\nğŸ¯ è¯·è¾“å…¥é€‰æ‹© (1-5): ").strip()
        
        if choice == '1':
            demo_interactive_mode()
        elif choice == '2':
            demo_batch_processing()
        elif choice == '3':
            demo_api_simulation()
        elif choice == '4':
            demo_integration_example()
        elif choice == '5':
            print("\nğŸ‘‹ æ„Ÿè°¢ä½¿ç”¨ï¼")
            break
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-5")

if __name__ == "__main__":
    main() 